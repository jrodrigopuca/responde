{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "a5e66c02",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "a5e66c02"
      },
      "outputs": [],
      "source": [
        "# Mini-GPT visual con Gradio en Colab\n",
        "\n",
        "# Paso 1: Instalación\n",
        "!pip install torch sentencepiece gradio --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "dc6f310c",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "dc6f310c"
      },
      "outputs": [],
      "source": [
        "# Paso 2: Importación\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import sentencepiece as spm\n",
        "import gradio as gr\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "70a98004",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "70a98004"
      },
      "outputs": [],
      "source": [
        "# Paso 3: Configuración\n",
        "BLOCK_SIZE = 8\n",
        "EMBED_DIM = 64\n",
        "N_HEADS = 2\n",
        "N_LAYERS = 2\n",
        "LEARNING_RATE = 1e-3\n",
        "MODEL_PREFIX = \"bpe\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "a2f0b317",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "a2f0b317"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "CORPUS = \"corpus.txt\"\n",
        "texto_entrenamiento = \"\"\"\n",
        "La luna se asoma detrás del monte,\n",
        "como un ojo que espía el silencio.\n",
        "El viento arrastra hojas secas,\n",
        "dibujando círculos en el aire.\n",
        "\n",
        "La noche canta con voz de agua,\n",
        "y las estrellas tiemblan en su lecho.\n",
        "Un farol parpadea en la distancia,\n",
        "como un suspiro que no quiere irse.\n",
        "\n",
        "Bajo un cielo bordado de sombra,\n",
        "las palabras se ocultan en el pecho.\n",
        "Todo calla, todo espera,\n",
        "como si el mundo respirara lento.\n",
        "\n",
        "Los árboles murmuran viejas historias,\n",
        "y el río sueña con su nacimiento.\n",
        "Una flor se abre en plena oscuridad,\n",
        "y el universo guarda el secreto.\n",
        "\"\"\"\n",
        "\n",
        "# Guardar corpus si no existe\n",
        "if not os.path.exists(CORPUS):\n",
        "    with open(CORPUS, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(texto_entrenamiento)\n",
        "\n",
        "# Entrenar tokenizador si no existe\n",
        "if not os.path.exists(f\"{MODEL_PREFIX}.model\"):\n",
        "    import sentencepiece as spm\n",
        "    spm.SentencePieceTrainer.Train(\n",
        "        input=CORPUS,\n",
        "        model_prefix=MODEL_PREFIX,\n",
        "        vocab_size=100,\n",
        "        model_type=\"bpe\"\n",
        "    )\n",
        "\n",
        "sp = spm.SentencePieceProcessor(model_file=f\"{MODEL_PREFIX}.model\")\n",
        "vocab_size = sp.get_piece_size()\n",
        "sp = spm.SentencePieceProcessor(model_file=f\"{MODEL_PREFIX}.model\")\n",
        "vocab_size = sp.get_piece_size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "2fba9255",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "2fba9255"
      },
      "outputs": [],
      "source": [
        "# Paso 5: Tokenización\n",
        "encode = lambda s: sp.encode(s, out_type=int)\n",
        "decode = lambda l: sp.decode(l)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "20dd8ce1",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "20dd8ce1"
      },
      "outputs": [],
      "source": [
        "# Paso 6: Modelo MiniGPT\n",
        "class SelfAttentionHead(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(EMBED_DIM, head_size, bias=False)\n",
        "        self.query = nn.Linear(EMBED_DIM, head_size, bias=False)\n",
        "        self.value = nn.Linear(EMBED_DIM, head_size, bias=False)\n",
        "        self.register_buffer(\"tril\", torch.tril(torch.ones(BLOCK_SIZE, BLOCK_SIZE)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "        wei = q @ k.transpose(-2, -1) / (C**0.5)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        v = self.value(x)\n",
        "        return wei @ v\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([SelfAttentionHead(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(num_heads * head_size, EMBED_DIM)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.proj(torch.cat([h(x) for h in self.heads], dim=-1))\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(EMBED_DIM, 4 * EMBED_DIM),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * EMBED_DIM, EMBED_DIM),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.sa = MultiHeadAttention(N_HEADS, EMBED_DIM // N_HEADS)\n",
        "        self.ffwd = FeedForward()\n",
        "        self.ln1 = nn.LayerNorm(EMBED_DIM)\n",
        "        self.ln2 = nn.LayerNorm(EMBED_DIM)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class MiniGPT(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_emb = nn.Embedding(vocab_size, EMBED_DIM)\n",
        "        self.pos_emb = nn.Embedding(BLOCK_SIZE, EMBED_DIM)\n",
        "        self.blocks = nn.Sequential(*[Block() for _ in range(N_LAYERS)])\n",
        "        self.ln_f = nn.LayerNorm(EMBED_DIM)\n",
        "        self.head = nn.Linear(EMBED_DIM, vocab_size)\n",
        "\n",
        "    def forward(self, idx):\n",
        "        B, T = idx.shape\n",
        "        tok_emb = self.token_emb(idx)\n",
        "        pos_emb = self.pos_emb(torch.arange(T, device=idx.device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.head(x)\n",
        "        return logits\n",
        "\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=10):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -BLOCK_SIZE:]\n",
        "            logits = self(idx_cond)[:, -1, :] / temperature\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            topk_probs, topk_idx = torch.topk(probs, top_k)\n",
        "            probs = torch.zeros_like(probs).scatter_(1, topk_idx, topk_probs)\n",
        "            probs = probs / probs.sum(dim=-1, keepdim=True)\n",
        "            next_id = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, next_id), dim=1)\n",
        "        return idx\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "5c5974ab",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "5c5974ab",
        "outputId": "620302e3-d397-4662-807f-970a44b5bc0d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modelo cargado desde minigpt_model.pt\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MiniGPT(\n",
              "  (token_emb): Embedding(100, 64)\n",
              "  (pos_emb): Embedding(8, 64)\n",
              "  (blocks): Sequential(\n",
              "    (0): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-1): 2 x SelfAttentionHead(\n",
              "            (key): Linear(in_features=64, out_features=32, bias=False)\n",
              "            (query): Linear(in_features=64, out_features=32, bias=False)\n",
              "            (value): Linear(in_features=64, out_features=32, bias=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=64, out_features=64, bias=True)\n",
              "      )\n",
              "      (ffwd): FeedForward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (1): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-1): 2 x SelfAttentionHead(\n",
              "            (key): Linear(in_features=64, out_features=32, bias=False)\n",
              "            (query): Linear(in_features=64, out_features=32, bias=False)\n",
              "            (value): Linear(in_features=64, out_features=32, bias=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=64, out_features=64, bias=True)\n",
              "      )\n",
              "      (ffwd): FeedForward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (ln_f): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "  (head): Linear(in_features=64, out_features=100, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "# Paso 7: Cargar o entrenar modelo MiniGPT\n",
        "model = MiniGPT()\n",
        "\n",
        "if not os.path.exists(\"minigpt_model.pt\"):\n",
        "    print(\"Entrenando modelo MiniGPT...\")\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "    data = torch.tensor(encode(texto_entrenamiento), dtype=torch.long)\n",
        "    def get_batch(data):\n",
        "        ix = torch.randint(len(data) - BLOCK_SIZE, (4,))\n",
        "        x = torch.stack([data[i:i+BLOCK_SIZE] for i in ix])\n",
        "        y = torch.stack([data[i+1:i+BLOCK_SIZE+1] for i in ix])\n",
        "        return x, y\n",
        "    for step in range(300):\n",
        "        x, y = get_batch(data)\n",
        "        logits = model(x)\n",
        "        loss = F.cross_entropy(logits.view(-1, vocab_size), y.view(-1))\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if step % 50 == 0:\n",
        "            print(f\"Paso {step} - Pérdida: {loss.item():.4f}\")\n",
        "    torch.save(model.state_dict(), \"minigpt_model.pt\")\n",
        "    print(\"Modelo entrenado y guardado como minigpt_model.pt\")\n",
        "else:\n",
        "    model.load_state_dict(torch.load(\"minigpt_model.pt\"))\n",
        "    print(\"Modelo cargado desde minigpt_model.pt\")\n",
        "model = MiniGPT()\n",
        "model.load_state_dict(torch.load(\"minigpt_model.pt\"))\n",
        "model.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "16b21383",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "16b21383",
        "outputId": "579ba1ec-1f28-472e-b9b6-a4e46491b823",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://e6961eca9fb3b39e03.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://e6961eca9fb3b39e03.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "def generar(input_text, tokens, temp, topk):\n",
        "    context = torch.tensor([encode(input_text)], dtype=torch.long)\n",
        "    out = model.generate(context, max_new_tokens=tokens, temperature=temp, top_k=topk)[0].tolist()\n",
        "    token_str = ', '.join(str(t) for t in out)\n",
        "    texto = decode(out)\n",
        "    token_pieces = ', '.join(sp.id_to_piece(t) for t in out)\n",
        "    return f\"Tokens: [{token_str}]\\nSubpalabras: [{token_pieces}]\\n\\nTexto generado:\\n{texto}\"\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=generar,\n",
        "    inputs=[\n",
        "        gr.Textbox(label=\"Texto inicial\"),\n",
        "        gr.Slider(5, 100, value=20, step=1, label=\"Cantidad de tokens a generar\"),\n",
        "        gr.Slider(0.1, 2.0, value=1.0, step=0.1, label=\"Creatividad (Temperature)\"),\n",
        "        gr.Slider(1, 50, value=10, step=1, label=\"Top-K (limita opciones por paso)\")\n",
        "    ],\n",
        "    outputs=\"text\",\n",
        "    title=\"Mini-GPT Generador Poético\"\n",
        ")\n",
        "\n",
        "iface.launch(share=True)"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}