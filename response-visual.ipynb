{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e66c02",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Mini-GPT visual con Gradio en Colab\n",
    "\n",
    "# Paso 1: Instalación\n",
    "!pip install torch sentencepiece gradio --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6f310c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Paso 2: Importación\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import sentencepiece as spm\n",
    "import gradio as gr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a98004",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Paso 3: Configuración\n",
    "BLOCK_SIZE = 8\n",
    "EMBED_DIM = 64\n",
    "N_HEADS = 2\n",
    "N_LAYERS = 2\n",
    "LEARNING_RATE = 1e-3\n",
    "MODEL_PREFIX = \"bpe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f0b317",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Paso 4: Cargar modelo de sentencepiece entrenado\n",
    "sp = spm.SentencePieceProcessor(model_file=f\"{MODEL_PREFIX}.model\")\n",
    "vocab_size = sp.get_piece_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fba9255",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Paso 5: Tokenización\n",
    "encode = lambda s: sp.encode(s, out_type=int)\n",
    "decode = lambda l: sp.decode(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dd8ce1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Paso 6: Modelo MiniGPT\n",
    "class SelfAttentionHead(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(EMBED_DIM, head_size, bias=False)\n",
    "        self.query = nn.Linear(EMBED_DIM, head_size, bias=False)\n",
    "        self.value = nn.Linear(EMBED_DIM, head_size, bias=False)\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(BLOCK_SIZE, BLOCK_SIZE)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        wei = q @ k.transpose(-2, -1) / (C**0.5)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        v = self.value(x)\n",
    "        return wei @ v\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([SelfAttentionHead(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(num_heads * head_size, EMBED_DIM)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.proj(torch.cat([h(x) for h in self.heads], dim=-1))\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(EMBED_DIM, 4 * EMBED_DIM),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * EMBED_DIM, EMBED_DIM),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.sa = MultiHeadAttention(N_HEADS, EMBED_DIM // N_HEADS)\n",
    "        self.ffwd = FeedForward()\n",
    "        self.ln1 = nn.LayerNorm(EMBED_DIM)\n",
    "        self.ln2 = nn.LayerNorm(EMBED_DIM)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class MiniGPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(vocab_size, EMBED_DIM)\n",
    "        self.pos_emb = nn.Embedding(BLOCK_SIZE, EMBED_DIM)\n",
    "        self.blocks = nn.Sequential(*[Block() for _ in range(N_LAYERS)])\n",
    "        self.ln_f = nn.LayerNorm(EMBED_DIM)\n",
    "        self.head = nn.Linear(EMBED_DIM, vocab_size)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_emb(idx)\n",
    "        pos_emb = self.pos_emb(torch.arange(T, device=idx.device))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)\n",
    "        return logits\n",
    "\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=10):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -BLOCK_SIZE:]\n",
    "            logits = self(idx_cond)[:, -1, :] / temperature\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            topk_probs, topk_idx = torch.topk(probs, top_k)\n",
    "            probs = torch.zeros_like(probs).scatter_(1, topk_idx, topk_probs)\n",
    "            probs = probs / probs.sum(dim=-1, keepdim=True)\n",
    "            next_id = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, next_id), dim=1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5974ab",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Paso 7: Cargar modelo entrenado\n",
    "model = MiniGPT()\n",
    "model.load_state_dict(torch.load(\"minigpt_model.pt\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b21383",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Paso 8: Interfaz Gradio\n",
    "\n",
    "def generar(input_text, tokens, temp, topk):\n",
    "    context = torch.tensor([encode(input_text)], dtype=torch.long)\n",
    "    out = model.generate(context, max_new_tokens=tokens, temperature=temp, top_k=topk)[0].tolist()\n",
    "    return decode(out)\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=generar,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Texto inicial\"),\n",
    "        gr.Slider(5, 100, value=20, step=1, label=\"Cantidad de tokens a generar\"),\n",
    "        gr.Slider(0.1, 2.0, value=1.0, step=0.1, label=\"Creatividad (Temperature)\"),\n",
    "        gr.Slider(1, 50, value=10, step=1, label=\"Top-K (limita opciones por paso)\")\n",
    "    ],\n",
    "    outputs=\"text\",\n",
    "    title=\"Mini-GPT Generador Poético\"\n",
    ")\n",
    "\n",
    "iface.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
