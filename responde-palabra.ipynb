{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "befcf8ed",
      "metadata": {
        "id": "befcf8ed"
      },
      "source": [
        "# Mini-GPT desde cero en Google Colab\n",
        "\n",
        "\"\"\"\n",
        "Este notebook implementa un modelo GPT en miniatura desde cero con PyTorch.\n",
        "La idea es entender y experimentar con los conceptos básicos de atención,\n",
        "tokenización, y generación de texto sin necesidad de una gran computadora.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5a22939",
      "metadata": {
        "id": "c5a22939"
      },
      "outputs": [],
      "source": [
        "# Paso 1: Instalación de dependencias\n",
        "!pip install torch matplotlib --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "850ee7d9",
      "metadata": {
        "id": "850ee7d9"
      },
      "outputs": [],
      "source": [
        "# Paso 2: Imports iniciales\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41b0df5e",
      "metadata": {
        "id": "41b0df5e"
      },
      "outputs": [],
      "source": [
        "# Paso 3: Dataset pequeño de ejemplo (versión por palabras)\n",
        "texto = \"\"\"\n",
        "La luna brilla sobre el mar.\n",
        "Las estrellas miran en silencio.\n",
        "La noche canta con voz serena.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7587b9b2",
      "metadata": {
        "id": "7587b9b2"
      },
      "outputs": [],
      "source": [
        "# Paso 4: Tokenización por palabras\n",
        "palabras = texto.lower().replace(\"\\n\", \" \").split()\n",
        "vocab = sorted(set(palabras))\n",
        "vocab_size = len(vocab)\n",
        "stoi = { w:i for i,w in enumerate(vocab) }\n",
        "itos = { i:w for i,w in enumerate(vocab) }\n",
        "encode = lambda s: [stoi[w] for w in s.lower().replace(\"\\n\", \" \").split()]\n",
        "decode = lambda l: ' '.join([itos[i] for i in l])\n",
        "\n",
        "data = torch.tensor(encode(texto), dtype=torch.long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cadb95d9",
      "metadata": {
        "id": "cadb95d9"
      },
      "outputs": [],
      "source": [
        "# Paso 5: Hiperparámetros\n",
        "BLOCK_SIZE = 4  # menos contexto, porque usamos palabras\n",
        "BATCH_SIZE = 4\n",
        "EMBED_DIM = 32\n",
        "N_HEADS = 2\n",
        "N_LAYERS = 2\n",
        "EPOCHS = 300\n",
        "LEARNING_RATE = 1e-3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8478f32d",
      "metadata": {
        "id": "8478f32d"
      },
      "outputs": [],
      "source": [
        "# Paso 6: Batching\n",
        "def get_batch(data):\n",
        "    ix = torch.randint(len(data) - BLOCK_SIZE, (BATCH_SIZE,))\n",
        "    x = torch.stack([data[i:i+BLOCK_SIZE] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+BLOCK_SIZE+1] for i in ix])\n",
        "    return x, y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fed12b9",
      "metadata": {
        "id": "8fed12b9"
      },
      "outputs": [],
      "source": [
        "# Paso 7: Modelo MiniGPT\n",
        "class SelfAttentionHead(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(EMBED_DIM, head_size, bias=False)\n",
        "        self.query = nn.Linear(EMBED_DIM, head_size, bias=False)\n",
        "        self.value = nn.Linear(EMBED_DIM, head_size, bias=False)\n",
        "        self.register_buffer(\"tril\", torch.tril(torch.ones(BLOCK_SIZE, BLOCK_SIZE)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "        wei = q @ k.transpose(-2, -1) / (C**0.5)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        v = self.value(x)\n",
        "        return wei @ v\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([SelfAttentionHead(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(num_heads * head_size, EMBED_DIM)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.proj(torch.cat([h(x) for h in self.heads], dim=-1))\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(EMBED_DIM, 4 * EMBED_DIM),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * EMBED_DIM, EMBED_DIM),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.sa = MultiHeadAttention(N_HEADS, EMBED_DIM // N_HEADS)\n",
        "        self.ffwd = FeedForward()\n",
        "        self.ln1 = nn.LayerNorm(EMBED_DIM)\n",
        "        self.ln2 = nn.LayerNorm(EMBED_DIM)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class MiniGPT(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_emb = nn.Embedding(vocab_size, EMBED_DIM)\n",
        "        self.pos_emb = nn.Embedding(BLOCK_SIZE, EMBED_DIM)\n",
        "        self.blocks = nn.Sequential(*[Block() for _ in range(N_LAYERS)])\n",
        "        self.ln_f = nn.LayerNorm(EMBED_DIM)\n",
        "        self.head = nn.Linear(EMBED_DIM, vocab_size)\n",
        "\n",
        "    def forward(self, idx):\n",
        "        B, T = idx.shape\n",
        "        tok_emb = self.token_emb(idx)\n",
        "        pos_emb = self.pos_emb(torch.arange(T, device=idx.device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.head(x)\n",
        "        return logits\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -BLOCK_SIZE:]\n",
        "            logits = self(idx_cond)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            next_id = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, next_id), dim=1)\n",
        "        return idx\n",
        "\n",
        "# Crear modelo y optimizador\n",
        "model = MiniGPT()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7cf34e40",
      "metadata": {
        "id": "7cf34e40"
      },
      "outputs": [],
      "source": [
        "# Paso 8: Entrenamiento con registro de pérdida\n",
        "losses = []\n",
        "for step in range(EPOCHS):\n",
        "    x, y = get_batch(data)\n",
        "    logits = model(x)\n",
        "    loss = F.cross_entropy(logits.view(-1, vocab_size), y.view(-1))\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if step % 50 == 0:\n",
        "        print(f\"Paso {step} - Pérdida: {loss.item():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6af37f8e",
      "metadata": {
        "id": "6af37f8e"
      },
      "outputs": [],
      "source": [
        "# Paso 9: Graficar pérdida de entrenamiento\n",
        "plt.plot(losses)\n",
        "plt.title(\"Pérdida durante el entrenamiento\")\n",
        "plt.xlabel(\"Paso\")\n",
        "plt.ylabel(\"Pérdida\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34099ad8",
      "metadata": {
        "id": "34099ad8"
      },
      "outputs": [],
      "source": [
        "# Paso 10: Guardar modelo\n",
        "PATH = \"minigpt_model.pt\"\n",
        "torch.save(model.state_dict(), PATH)\n",
        "print(\"Modelo guardado en minigpt_model.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c1d3093",
      "metadata": {
        "id": "1c1d3093"
      },
      "outputs": [],
      "source": [
        "# Paso 11: Cargar modelo (opcional)\n",
        "# model.load_state_dict(torch.load(PATH))\n",
        "# print(\"Modelo cargado desde disco\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c35f1b77",
      "metadata": {
        "id": "c35f1b77"
      },
      "outputs": [],
      "source": [
        "# Paso 12: Interfaz interactiva para generación en Colab\n",
        "print(\"Consultá al modelo ingresando algunas palabras:\")\n",
        "entrada = input(f\"Ingrese un inicio de frase (máx {BLOCK_SIZE} palabras): \")\n",
        "contexto = encode(entrada.strip().split())\n",
        "context_tensor = torch.tensor([contexto], dtype=torch.long)\n",
        "salida = model.generate(context_tensor, 10)[0].tolist()\n",
        "print(\"--- Texto generado ---\")\n",
        "print(decode(salida))"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}